---
title: "Autor: Sergi Sánchez Romero y Lucia Blanc Velázquez"
author: "Diciembre 2023"
date: ""
output:
  html_document: 
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: PEC-header.html
  pdf_document: 
    highlight: zenburn
    toc: yes
    always_allow_html: true  
  word_document: default
---

******
<h2 style="background-color: #0E4D83; color: white;">1. Introducción</h2> 
******


<h3 style="background-color: #2677AF; color: white;">1.1. Presentación</h3>

En esta práctica se elabora un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y usar las herramientas de integración, limpieza, validación y análisis de los mismos.


<h3 style="background-color: #2677AF; color: white;">1.2. Objetivos</h3>

● Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.

● Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.

● Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.

● Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.

● Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.

● Desarrollar las habilidades de aprendizaje que les permitan continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.

● Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos.






******
<h2 style="background-color: #0E4D83; color: white;">2. Descripción del dataset: "Heart Attack Analysis & Prediction dataset"</h2> 
******
El conjunto de datos escogido para esta práctica, se titula: “Heart Attack Analysis & Prediction dataset”, el cual tiene como objetivo detectar aquellos factores que pueden actuar como potenciales precursores de las enfermedades cardiovasculares, y así ayudar a la detección y gestión temprana mediante la creación de un modelo. 
En el conjunto de datos se contemplan un total de 14 características importantes, tanto numéricas como categóricas que pueden ayudar a la predicción de desarrollar o no una enfermedad cardíaca. Según la información encontrada, se sabe que las enfermedades cardiovasculares ocupan un porcentaje del 31% en relación a las muertes que se producen en el mundo cada año, por lo que supone una de las causas principales de muerte. La variable 'output', consiste en valores de 0 o 1 que indican si una persona tiene más probabilidad de sufrir un infarto (1) o menor probabilidad (0).
Es importane tener en cuenta cuales son los atributos que pueden conllevar a un mayor riesgo cardiovascular o al desarrollo de enfermedad cardiovascular, por lo que, el objetivo es predecir que variables influyen más en este desarrollo.



A continuación, realizamos la descripción de las variables que hay en el dataset "Heart Attack Analysis & Prediction dataset", usando la información encontrada en la web [Kaggle datasets] (https://www.kaggle.com/datasets), concretamente en el siguiente enlace: https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset 

** Características de las variales incluidas:**

+ **age**: Edad del paciente

+ **sex** : Sexo del paciente (F=0; M=1)

+ **cp** : Tipo dolor torácico

  -  **Value 1** : Angina típica (TA)

  - **Value 2** : Angina atípica (ATA)

  - **Value 3** : Dolor no-anginal (NAP)

  - **Value 4** : Asintomático (ASY)

+ **trtbps** : Presión arterial en reposo (in mm Hg)

+ **chol** : Colesterol en mg/dl obtenido a través del sensor de IMC 

+ **fbs** : (Glucemia en ayunas > 120 mg/dl) (1 = true; 0 = false)

+ **restecg** : Resultados del electrocardiograma en reposo 

  - **Value 0** : Normal

  - **Value 1** : Presentar anomalías de la onda ST-T (inversión de la onda T y/o elevación o depresión del ST de > 0,05 mV)

  - **Value 2** : Hipertrofia ventricular izquierda probable o definida según los criterios de Estes

+ **thalachh** : Frecuencia cardiaca máxima alcanzada 

+ **exng** : Angina inducida por esfuerzo (1 = yes; 0 = no)

+ **oldpeak** : Pico previo

+ **slp** : Pendiente del segmento ST máximo del ejercicio

+ **caa** : Número de buques principales (0-3)

+ **thall** : Tasa de mortalidad

+ **output** : 0= menor probabilidad de infarto 1= mayor probabilidad de infarto 



\newpage

******
<h2 style="background-color: #0E4D83; color: white;">3. Integración y selección de los datos de interés a analizar</h2> 
******
Puede ser el resultado de adicionar diferentes datasets o una subselección útil de los datos originales, en base al objetivo que se quiera conseguir.

<h3 style="background-color: #0E4D83; color: white;">3.1. Carga de Librerías</h3> 
Primero de todo, cargamos las librerías que vamos a usar durante la práctica
```{r message= FALSE, warning=FALSE}
if (!require('csv')) install.packages('csv');library(csv)
if (!require('dplyr')) install.packages('dplyr');library(dplyr)
if (!require('ggplot2')) install.packages('ggplot2');library(ggplot2)
if (!require('reshape')) install.packages('reshape');library(reshape)
if (!require('plotly')) install.packages('plotly');library(plotly)
if (!require('plyr')) install.packages('plyr');library(plyr)
if (!require('Stat2Data')) install.packages('Stat2Data');library(Stat2Data)
if (!require('corrplot')) install.packages('corrplot');library(corrplot)
if (!require('Matrix')) install.packages('matrix');library(Matrix)
if (!require('patchwork')) install.packages('patchwork');library(patchwork)
if (!require('ggcorrplot')) install.packages('ggcorrplot');library(ggcorrplot)
if (!require('corrplot')) install.packages('ggcorrplot');library(corrplot)
if (!require('DataExplorer'))install.packages('DataExplorer');library(DataExplorer)
if (!require('psych'))install.packages('psych');library(psych)
if (!require('highcharter'))install.packages('highcharter');library(highcharter)
if (!require('tidyverse'))install.packages('tidyverse');library(tidyverse)
if (!require('GGally'))install.packages('GGally');library(GGally)
if (!require('htmltools'))install.packages('htmltools');library(htmltools)

# setwd
dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(dir)
```


<h3 style="background-color: #2677AF; color: white;">3.2. Carga del dataset</h3>
Cargamos los datos de la base de datos "heart" y tipificamos las variables que tiene el conjunto de datos como corresponde
```{r}
library(csv)
heart <- read.csv("heart.csv", header=T,sep=",")
```


<h3 style="background-color: #2677AF; color: white;">3.3. Observación del dataset</h3>
```{r}
# Mostramos los primeros registros del conjunto de dtos, con el fin de ver una aproximación de como es el conjunto y su estructura
head(heart, max(10))
str(heart)

# Obtener la cantidad de filas y columnas
dimensiones <- dim(heart)
num_filas <- dimensiones[1]
num_columnas <- dimensiones[2]

# Imprimir el mensaje
mensaje <- paste("El conjunto tiene", num_filas, "filas y", num_columnas, "columnas.")
print(mensaje)


# Obtenemos un primer vistazo estadístico de cada atirbuto
describeBy(heart)
```

<h2 style="background-color: #0E4D83; color: white;">4.Limpieza de los datos</h2>
Primero de todo, hacemos una matriz de correlaciones de todo el conjunto, para ver como son los datos
```{r}
library(corrplot)
# Convirtimos todas las variables a numéricas (asegúrate de que las variables sean numéricas)
heart_numeric <- sapply(heart, as.numeric)

 # Calculamos la matriz de correlación
correlation_tab <- cor(heart_numeric)

library(RColorBrewer)

# Definimos una nueva paleta de colores
new_col <- colorRampPalette(c("#0000CD", "#7D26CD", "#FFFFFF","#FF6347","#FF0000"))

# Crear la matriz de correlación con corrplot y personalización adicional
corrplot(correlation_tab, 
         method = "color", 
         tl.col = "black", 
         tl.srt = 30, 
         tl.cex = 0.6, 
         cl.cex = 0.6, 
         col = new_col(200), 
         addCoef.col = "black", 
         order = "AOE", 
         number.cex = 0.6)


```
**Observaciones**
Según la matriz de correlaciones inicial, presentada anteriormente, donde las correlaciones positivas se muestran en color rojo y las negativas en color azul fuerte podemos ver cuales son las variables con mayores intensidad de color, guiando así la significación de los coeficientes de correlación. 

- La variable de interés (output) se relaciona positivamente con slp, thalachh y cp, y negativamente con caa, oldpeal, exng y thall.
- No existe ningún tipo de relación entre la variable output y la variable chol ni la variable fbs.
- Hay una fuerte correlación negativa entre slp y oldpeak (-0.58), y una correlación positiva entre slp y thalachh. 
- Las variables cp y exng estan correlacionadas negativamente, igual que la edad (age) y la variable thalachh.




<h3 style="background-color: #2677AF; color: white;">4.1 Valores nulos y/o vacíos</h3>

Debido que algunas de las variables del conjunto de datos tienen tipos de datos incorrectos, debemos transformar los tipos de datos antes del análisis
```{r}
# Clasificamos las variables en numéricas o en categóricas
# Numéricas
heart <- heart%>%
  mutate_at(vars(age,trtbps,chol, thalachh, oldpeak), as.numeric)

# Categóricas
heart <- heart%>%
  mutate_at(vars(sex, cp, fbs, restecg, exng, slp, thall, caa, output), as.factor)

introduce(heart)
```
Según la tabla y el barplot creado, podemos ver como el conjunto de datos heart tiene 14 atributos y 303 observaciones, y contiene un total de 9 columnas discretas y 5 columnas continuas.


Con la librería DataExplorer vemos una vista general del conjunto de datos de análisis una vez seleccionados, basandonos en los valores faltantes, columnas discretas y continuas.
```{r}
plot_intro(heart, title = "Información Dataset")
```
Tal y como vemos en el barplot creado, podemos ver la distirbución del conjunto como en la tabla anterior y también observamos como no hay valores faltantes.



Ahora vamos a visualizar la información básica del conjunto de datos en función de la variable 'output' de interés
```{r}
# La variable output nos va indicar quien tiene o no una mayor probabilidad de sufrir un ataque al corazón, por lo que primero calculamos el porcentaje de pacientes que tienen mayor probabilidad y luego el resto

print("Porcentaje de personas con probabilidad de infarto")
round((sum(heart$output == 1)/nrow(heart)) * 100, 2)


print("Porcentaje de personas sin probabilidad de infarto")
round((sum(heart$output == 0)/nrow(heart))*100,2)
```



<h4 style="background-color: #2677AF; color: white;">4.2. Valores Duplicados</h4>
```{r}
# Valores duplicados
get_duplicates <- function(heart){
    total_rows = dim(heart)[1]
    unique_rows = dim(heart %>% group_by_all %>% count)[1]
    n_duplicates = (total_rows - unique_rows)
    cat('n duplicates -> ', n_duplicates)
}

get_duplicates(heart) # Vemos que hay un valor duplicado

heart = unique(heart)


cat('Eliminamos la fila duplicada')

get_duplicates(heart)
```
Ya no tenemos valores duplicados en el conjunto de datos




<h3 style="background-color: #2677AF; color: white;">4.3. Valores Extremos</h3>

Seguidamente es importante estudiar la posibilidad de valores outliers para las variables numéricas de la base de datos
```{r}
# Cargamos librerías necesarias
library(ggplot2)

# Reorganizamos el dataframe para plotear las variables seleccionadas
selected_vars <- heart[, c("age", "oldpeak", "chol", "thalachh", "trtbps")]
selected_vars <- stack(selected_vars)

# Agregamos la columna 'output' al dataframe para usarla en el color de los puntos
selected_vars$output <- heart$output

# Creamos un gráfico de cajas con puntos superpuestos para las variables numéricas seleccionadas
ggplot(selected_vars, aes(x = ind, y = values, color = as.factor(output))) +
  geom_boxplot(alpha = 0.7, outlier.size = 2) +  
  geom_jitter(alpha = 0.3, size = 0.5) +  
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  
  labs(x = "Variables", y = "Valores") +
  scale_color_brewer(palette = "Set1", name = "Probabilidad de Infarto", labels = c('0 - Menor Probabilidad', '1 - Mayor Probabilidad')) + 
  ggtitle("Gráfico de Cajas con Puntos - Variables Numéricas y Outliers")

```
**Observaciones**
Podemos ver como hay puntos muy alejados de las medias y de los boxplots creados en las variables chol, thalachh y trtbps, por lo que, vamos a estudiar si mediante una función creada según las desviaciones estandard, se eliminan estos valores y dejamos el conjunto de datos sin potenciales valores outliers.



<h3 style="background-color: #2677AF; color: white;">4.4. Limpieza de Valores Extremos </h3>
Para limpiar los valores atípicos (outliers) usamos un enfoque del rango intercuartílico (IQR), el cual es una medida de dispersión que se bada en la diferencia entre el tercer (Q3) y primer cuartil (Q1) del conjunto de datos
```{r}
# Creamos un dataframe de las variables de las cuales hemos de quitar outliers, excluyendo age y oldpeak
df_outliers<-as.data.frame(heart %>%
                  select("trtbps","thalachh","chol"))

# Función para quitar outliers
outliers <- function(x) { 
 # IQR
  Q1 <- quantile(x, probs=.25) 
  Q3 <- quantile(x, probs=.75) 
  iqr = Q3-Q1 
 
 # Rango Superior
 upper_limit = Q3 + (iqr*1.5) 
 # Rango Inferior 
 lower_limit = Q1 - (iqr*1.5) 
 
 x > upper_limit | x < lower_limit 
} 

# Quitamos valores atípicos
remove_outliers <- function(df_outliers, cols = names(df_outliers)) { 
  for (col in cols) { 
    df_outliers<- df_outliers[!outliers(df_outliers[[col]]),] 
  } 
  df_outliers 
}

# Una vez creada la función para quitar los outliers, creamos un nuevo conjutno de datos sin los outliers 
heart_clean<-remove_outliers(heart,c("trtbps","thalachh", "chol"))


# Observamos el cambio de las dimensiones
dim(heart_clean)
```
Los valores potenciales outliers han sido eliminados y ahora tenemos el conjunto de datos con un total de 287 filas y 14 variables. 


**Resultado de Limpieza de Outliers**
```{r}
# Reorganizamos el dataframe para plotear las variables seleccionadas
selected_vars <- heart_clean[, c("age", "oldpeak", "chol", "thalachh", "trtbps")]
selected_vars <- stack(selected_vars)

# Agregamos la columna 'output' al dataframe para usarla en el color de los puntos
selected_vars$output <- heart_clean$output

# Creamos un gráfico de cajas con puntos superpuestos para las variables numéricas seleccionadas
ggplot(selected_vars, aes(x = ind, y = values, color = as.factor(output))) +
  geom_boxplot(alpha = 0.7, outlier.size = 2) +  
  geom_jitter(alpha = 0.3, size = 0.5) +  
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  
  labs(x = "Variables", y = "Valores") +
  scale_color_brewer(palette = "Set1", name = "Probabilidad de Infarto", labels = c('0 - Menor Probabilidad', '1 - Mayor Probabilidad')) + 
  ggtitle("Gráfico de Cajas con Puntos - Variables Numéricas y Outliers")
```






<h3 style="background-color: #2677AF; color: white;">4.5. Comprobación de la normalidad y homogeneidad de la varianza.</h3>


<h4 style="background-color: #2677AF; color: white;">4.5.1 Comprobación de la normalidad</h4>

Para evaluar la normalidad de las variables seleccionadas, empleamos la prueba de Spahiro-Wilk
```{r}
# Usamos la prueba de Shapiro-Wilk para verificar la normalidad de cada variable numérica
variables <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
resultados_shapiro <- lapply(heart_clean[variables], shapiro.test)
names(resultados_shapiro) <- variables

# Verificar la estructura de las variables seleccionadas en el conjunto de datos 'heart'
str(heart_clean[, variables])
# Mostramos los resultados
print(resultados_shapiro)

```
Los resultados de la normalidad muestran que para cada una de las variables numéricas del conjunto de datos, excepto la variable chol, hay evidencia suficiente para rechazar la hipótesis nula y afirmar que los datos no siguen una distribución normal, pero como las observaciones son mayores a 30 se puede asumir.


**Normalizamos los datos**
A cotninuación normalizamos los datos con valores entre -1 y 1 para poder hacer análisis posteriores
```{r}
heart_clean
# Normalizamos todas las columnas de heart_clean excepto la columna 'output' (si existe)
heart_clean_n<-heart_clean

if ("output" %in% colnames(heart_clean_n)) {
  output_column <- heart_clean_n$output  # Guarda la columna 'output'
  heart_clean_n <- heart_clean_n[, !names(heart_clean_n) %in% "output"]  # Elimina la columna 'output' para normalizar
}

# Convertir todas las columnas a tipo numérico
heart_clean_numeric <- as.data.frame(lapply(heart_clean_n, as.numeric))


# Normalizamos los datos
heart_norm <- as.data.frame(scale(heart_clean_numeric))

# Agregamos la columna 'output'
if (exists("output_column")) {
  heart_norm$output <- output_column
}
```


<h4 style="background-color: #2677AF; color: white;">4.5.2 Comprobación de la varianza</h4>
Calculamos las varianzas de las variables numéricas 'age', 'trtbps', 'chol', 'thalachh', y 'oldpeak', del conjunto, agrupadas según la probabilidad de sufrir o no un infarto. 
```{r}
# Calculamos la varianza para cada variable según los niveles de 'output'
for (variable in variables) {
  var_por_output <- tapply(heart_norm[[variable]], heart_norm$output, var)
  mensaje <- paste("Varianza de", variable, "según 'output':")
  print(mensaje)
  print(var_por_output)
}

# Variables a analizar
variables <- c('age', 'trtbps', 'chol', 'thalachh', 'oldpeak')

# Calculamos la varianza para cada variable según los niveles de 'output'
resultados_var <- aggregate(. ~ output, data = heart_norm[, c('output', variables)], var)
print(resultados_var)

```
Estos resultado muestran la media de cada variable numérica (normalizada) para dos grupos distintos definidos por los niveles '0' y '1' de la variable 'output', los cuales indican menor o mayor probabilidad de ataque cardíaco.








<h2 style="background-color: #2677AF; color: white;">5. Análisis de Datos</h2>


<h3 style="background-color: #2677AF; color: white;">5.1. Análisis Univariado</h3>


<h4 style="background-color: #2677AF; color: white;">5.1.1 Variables Categóricas</h4>

Primero discretizaremos las variables categóricas, asignando a cada valor la correspondiente definición de la variable
```{r}
# Hacemos cópia del conjunto para usarlo solamente en este análisis:
heart_discr<-heart_clean

# Sexo del paciente (sex)
heart_discr$sex <- ifelse(heart_discr$sex == 0, "Mujer", "Hombre")

# Dolor Torácico (cp)
heart_discr$cp <- factor(heart_discr$cp, levels = c(1, 2, 3, 4), labels = c("Angina Típica", "Angina Atípica", "No Anginal", "Asintomático"))

# Resultados del Electrocardiograma en Reposo (restecg)
heart_discr$restecg <- factor(heart_discr$restecg, levels = c(0, 1, 2), labels = c("Normal", "Anomalías ST-T", "Hipertrofia ventricular"))

# Angina Inducida por Esfuerzo (exng)
heart_discr$exng <- ifelse(heart_discr$exng == 1, "Si", "No")

# Número de Buques Principales (caa) (0-3)
heart_discr$caa <- as.character(heart_discr$caa)

# Glucemia en Ayunas (fbs)
heart_discr$fbs <- ifelse(heart_discr$fbs == 1, "Verdadero", "Falso")

# Pendiente del Segmento ST Máximo del Ejercicio (slp)
heart_discr$slp <- factor(heart_discr$slp, levels = c(0, 1, 2), labels = c("Tipo 0", "Tipo 1", "Tipo 2"))

# Tasa de Mortalidad (thall)
heart_discr$thall <- factor(heart_discr$thall, levels = c(0, 1, 2, 3), labels = c("Thal0", "Thal1", "Thal2", "Thal3"))



# Ouput/Target (0= menor probabilidad de infarto 1= mayor probabilidad de infarto)
heart_discr$output <- factor(heart_discr$output, levels = c(0, 1), labels = c("Menor probabilidad", "Mayor probabilidad"))
```



A continuación estudiamos la estadística básica de las variables categóricas del conjunto heart
```{r}
library(dplyr)
library(ggplot2)


par(mfrow = c(2, 2))

categorical_var <- list("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")

for (i in categorical_var) {
  plot_data <- as.data.frame(table(heart_discr[[i]], heart_discr$output))
  colnames(plot_data) <- c(i, "output", "Freq")
  
  plot <- ggplot(plot_data, aes_string(x = i, y = "Freq", fill = "output")) +
    geom_bar(stat = "identity", position = position_dodge()) +
    scale_fill_manual(values = c("#BA55D3", "#6699CC"), 
                      name = "Probabilidad de Infarto", 
                      labels = c('0-Menor', '1-Mayor')) +
    labs(x = i, y = "Número de Observaciones") +
    theme_minimal() +  # Cambio de tema a minimal
    theme(panel.background = element_rect(fill = "white"),  # Fondo blanco
          axis.line = element_line(color = "black"))  # Líneas de ejes negras
  
  print(plot)
}

```

**Observaciones**

Según el thall, el riesgo de infarto se alcanza en las personas con frecuencia cardíaca máxima(clase 2).
En la característica sexo, la clase 1 tiene más posibilidades de sufrir un infarto que la clase 0.

Las probabilidades de sufrir un infarto son mayores en la clase 0 de sexo.

Comparando con el análisis de correlación, la característica fbs muestra la menor correlación con la salida.

En caa, las personas con clase, son más propensas a sufrir un ataque al corazón que las personas con clase 4, 3 y 2.

Según la característica cp, las personas con dolor no anginoso tienen más probabilidades de sufrir un infarto que las personas con dolor anginoso atípico y típico.

En exng, las personas con clase 1 tienen altas probabilidades de riesgo de infarto, mientras que las personas con clase 0 son menos propensas al infarto.

La característica Slp muestra que la clase 0 tiene menos correlación con el resultado que las clases 1 y 2.





<h4 style="background-color: #2677AF; color: white;">5.1.2 Variables Numéricas</h4>
A continuación estudiamos la estadística básica de las variables numéricas del conjunto
```{r}
# Creamos un gráfico de pares con las variables numéricas del conjunto de datos "heart"
# Establecer opciones para el tamaño del gráfico
options(repr.plot.width = 20, repr.plot.height = 20)  

# Creamos un gráfico de pares con las variables numéricas del conjunto de datos "heart"
pair_plot <- ggpairs(heart_clean, columns = c("age", "trtbps", "chol", "thalachh", "oldpeak"),
                     aes(color = as.factor(output), alpha = 0.5),
                     lower = list(continuous = "smooth"),
                     palette = c('blue', 'red')) +  # Usamos la misma paleta de colores
  theme_bw() +
  theme(text = element_text(size = 8),
        panel.grid = element_blank(),
        legend.position = "right",
        legend.title = element_text(face = "bold")) +
  ggtitle("Variables Numéricas") +
  labs(color = "Output", alpha = "Transparencia")


# Convertimos el gráfico a un gráfico interactivo con plotly
interactive_plot <- ggplotly(pair_plot)

# Mostramos el gráfico interactivo
interactive_plot

```



<h3 style="background-color: #0E4D83; color: white;">5.2. Modelo No Supervisado: Clustering</h3>

Un modelo no supervisado de clustering busca agrupar datos similares en conjuntos o clústeres. En este caso, la distancia Manhattan es una medida de distancia utilizada en clustering para calcular la diferencia entre dos puntos en un espacio multidimensional.

En este caso, puede ser útil para la identificación de subgrupos de pacientes con perfiles de riesgo similares de enfermedad cardíaca, lo que podría ser útil para personalizar tratamientos, identificar factores de riesgo comunes o incluso guiar futuras investigaciones médicas.

Para poder aplicar el logaritmo, escalamos los datos y mostramos el dendograma para saber donde determinar el corte y escoger el número de clústers.
```{r}
heart_norm <- as.data.frame(sapply(heart_norm, as.numeric))

# Escalamos los datos
heart_scale<-scale(heart_norm)

#Calculamos la distancia manhattan
dist_manh<-dist(heart_scale, method = 'manhattan')

#A continuación usamos la variable creada para minimizar las diferencias dentro de los conglomerados mediante el método Ward
hcluster<-hclust(dist_manh, method = "ward.D")
hcluster

```

```{r}
library(dendextend)
dendo<-as.dendrogram(hcluster)
dendo_k<-find_k(dendo)
plot(dendo_k)

# Observamos como el número de clusters obtenido es 2
plot(color_branches(dendo,k=dendo_k$nc)) #Hemos obtenido que el nombre de clusters será 2


# Creamos la matriz de confusión
grupos<-cutree(hcluster,k=2)
table(grupos,heart_clean$output)
```
Mediante la matriz de confusión creada, extraeremos el número de casos que se han clasificado correctamente y el porcentaje de precisión del modelo

```{r}
cat("Número de observaciones clasificadas correctamente =",121+121, "\n")
cat("Número de observaciones clasificadas incorrectamente =", 8+37, "\n")
cat("Precisión del modelo:",(242/(242+45))*100, "%\n")
```
La precisión del modelo es de un 84.32%.



A continuación, visualizamos el cluster con la función fviz_cluster:
```{r}
library(cluster)
library(factoextra)
pam.res <- pam(heart_scale, 2)

fviz_cluster(pam.res, geom = "point", ellipse.type = "norm",
             show.clust.cent = TRUE,star.plot = TRUE)+
  labs(title = "Resultados clustering K-means")+ theme_bw()

```

Con una precisión del modelo del 84.33%, se puede decir que este modelo de clasificación logra predecir correctamente alrededor del 84% de los casos en el conjunto de datos evaluado. Esto indica una capacidad razonablemente buena para predecir la enfermedad cardíaca en función de las características utilizadas en el modelo. Sin embargo, en un futuro también sería valioso evaluar otras métricas de rendimiento para obtener una comprensión más completa de su eficacia, como la sensibilidad, la especificidad u otras métricas según el contexto médico específico.





<h3 style="background-color: #0E4D83; color: white;">5.3. Modelo Supervisado: Regressión Logística</h3>
Este análisis busca entender cómo diferentes variables pueden influir en una variable de salida específica. En este caso, se trata de predecir un cierto resultado,es decir, si alguien tiene cierta enfermedad cardíaca o no (ouput).
```{r}
# Codificamos la variable objetivo como factor
heart_clean$output <- factor(heart_clean$output)

heart_clean <- as.data.frame(sapply(heart_clean, as.numeric))

# Reemplazamos los valores 2 por 0 en la columna "output"
heart_clean$output[heart_clean$output == 1] <- 0
heart_clean$output[heart_clean$output == 2] <- 1

# Dividimos el conjunto de datos en conjunto de entrenamiento y conjunto de prueba
library(caTools)
set.seed(123)
split = sample.split(heart_clean$output, SplitRatio = 0.75)
training_set = subset(heart_clean, split == TRUE)
test_set = subset(heart_clean, split == FALSE)

# Escalamos de características
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])

# Ajustamos la regresión logística al conjunto de entrenamiento
classifier = glm(formula = output ~ age+ sex + cp + trtbps+chol+fbs+restecg+thalachh+exng+oldpeak+slp+caa+thall,
                 family = binomial,
                 data = training_set)
summary(classifier)

# Predecimos los resultados del conjunto de prueba
prob_pred = predict(classifier, type = 'response', newdata = test_set[-14])
y_pred = ifelse(prob_pred > 0.5, 1, 0)

# Creamos la Matriz de Confusión
cm = table(test_set[, 14], y_pred > 0.5)
```


El modelo de regresión logística fue construido para predecir la probabilidad de ocurrencia de enfermedades cardíacas (output) en función de varias variables predictoras (age, sex, cp, trtbps, chol, fbs, restecg, thalachh, exng, oldpeak, slp, caa, thall).

+ **age (Edad)**: No se encuentra una relación significativa (coeficiente 0.028971, p-valor 0.913314) entre la edad y la probabilidad de enfermedades cardíacas en este modelo.
+ **sex (Género)**: Existe una relación significativa y negativa (coeficiente -0.844252, p-valor 0.001316) entre el género y la probabilidad de enfermedades cardíacas. Las mujeres tienden a tener una menor probabilidad de padecer enfermedades cardíacas en comparación con los hombres.
+ **cp (Tipo de dolor torácico)**: Se encuentra una relación positiva y significativa (coeficiente 0.784661, p-valor 0.000587). Mayores niveles de este tipo de dolor se asocian con un aumento en la probabilidad de enfermedades cardíacas.
+ **trtbps (Presión arterial en reposo), chol (Colesterol en suero), fbs (Azúcar en sangre en ayunas), restecg (Resultados electrocardiográficos en reposo) y thalachh (Frecuencia cardíaca máxima alcanzada)**: No se encuentran asociaciones significativas con la probabilidad de enfermedades cardíacas en este modelo.
+ **exng (Angina inducida por el ejercicio) y slp (Pendiente del segmento ST máximo del ejercicio)**: Aunque no son estadísticamente significativas al 95%, exng muestra una relación negativa y slp indica una posible asociación positiva con la probabilidad de enfermedades cardíacas.
+ **oldpeak (Depresión del ST inducida por el ejercicio)**: Existe una relación negativa significativa (coeficiente -0.726294, p-valor 0.016401). Mayor depresión del ST se asocia con una disminución en la probabilidad de enfermedades cardíacas.
caa (Número de vasos sanguíneos principales) y thall (Resultado de prueba de esfuerzo cardíaco): Muestran asociaciones significativas con la probabilidad de enfermedades cardíacas. Caa tiene una relación negativa y thall tiene una relación negativa con la probabilidad de enfermedades cardíacas.

En resumen, las variables más influyentes para predecir la ocurrencia de enfermedades cardíacas en este modelo son "cp" (Tipo de dolor torácico), "sex" (Género), "caa" (Número de vasos sanguíneos principales) y "thall" (Resultado de prueba de esfuerzo cardíaco). 

El AIC del modelo es 176.91, lo que sugiere que este modelo podría mejorar con ajustes adicionales o la inclusión de más variables predictoras. Además, la deviance residual es significativamente menor que la deviance nula, indicando que el modelo con las variables predictoras explica parte de la variabilidad en la variable de salida (enfermedades cardíacas).




<h2 style="background-color: #0E4D83; color: white;">Resolución del problema</h2>

La creación y evaluación del modelo de clasificación muestran una precisión del 84.33%, lo que indica una capacidad razonablemente buena para predecir la ocurrencia de enfermedades cardíacas en función de las variables utilizadas. Este nivel de precisión es prometedor y señala la relevancia de las características como predictores de la enfermedad.

Entre estas variables, "cp" (Tipo de dolor torácico), "sex" (Género), "caa" (Número de vasos sanguíneos principales) y "thall" (Resultado de prueba de esfuerzo cardíaco) destacan como las más influyentes para identificar la presencia de enfermedades cardíacas en este contexto.

No obstante, es vital destacar que la evaluación de más métricas de rendimiento, como la sensibilidad y la especificidad, podría proporcionar una imagen más completa de la eficacia del modelo, especialmente en un contexto médico específico. Además, considerar la inclusión de más variables o refinamientos para mejorar el modelo es clave, ya que el AIC sugiere que aún hay espacio para ajustes adicionales.

En cuanto a responder al problema, estos resultados proporcionan una comprensión inicial de qué variables pueden estar relacionadas con las enfermedades cardíacas, pero hay áreas que podrían necesitar más investigación o ajustes adicionales en el modelo. Se pueden formular hipótesis preliminares sobre las variables más influyentes, pero para una respuesta más completa al problema, se requeriría un análisis más detallado y quizás la inclusión de más variables predictoras o datos adicionales.